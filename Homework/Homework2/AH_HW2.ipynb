{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1\n",
    "#### DS7337: Natural Language Processing, Fall 2022\n",
    "By: Allen Hoskins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import text\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,minmax_scale\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Method to get the Vocab Size and normalize the score\n",
    "\n",
    "def n_vocab_size(*arg):\n",
    "    vocab_size = np.array([])\n",
    "    vocab_size_norm = np.array([])\n",
    "    \n",
    "    #### Getting the Vocab Size\n",
    "    for text in arg:\n",
    "        vocab_size = np.append(vocab_size,len(set(text)))\n",
    "    \n",
    "    #### Normalizing using the formula \n",
    "    for vsize in vocab_size:\n",
    "        vocab_size_norm = np.append(vocab_size_norm,(vsize - vocab_size.min()) /\n",
    "                                                    (vocab_size.max() - vocab_size.min()))\n",
    "    \n",
    "    #### Normalizing using sklearn preprocessing \n",
    "    vocab_size_norm_sklearn = minmax_scale(vocab_size, feature_range=(0,1), axis=0)\n",
    "    \n",
    "    return(vocab_size,vocab_size_norm,vocab_size_norm_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = n_vocab_size(text1,text2,text3,text4,text5,text6,text7,text8,text9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Values by using the formula\n",
      "- 1.0\n",
      "- 0.31440496457795597\n",
      "- 0.09231698610577187\n",
      "- 0.48970289417321106\n",
      "- 0.2722829370091713\n",
      "- 0.05810313581196112\n",
      "- 0.6205722444944808\n",
      "- 0.0\n",
      "- 0.31297709923664124\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalized Values by using the formula\", *vocab_size[1],sep='\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Values by using sklearn\n",
      "- 0.9999999999999999\n",
      "- 0.3144049645779559\n",
      "- 0.09231698610577188\n",
      "- 0.489702894173211\n",
      "- 0.2722829370091713\n",
      "- 0.05810313581196112\n",
      "- 0.6205722444944807\n",
      "- 0.0\n",
      "- 0.3129770992366412\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalized Values by using sklearn\", *vocab_size[2], sep='\\n- ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: \n",
    ">After consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW1_URL = 'https://www.gutenberg.org/cache/epub/24644/pg24644.txt'\n",
    "AUSTEN_EMMA = gutenberg.words('austen-emma.txt')\n",
    "AUSTEN_PERSUASION = gutenberg.words('austen-persuasion.txt')\n",
    "AUSTEN_SENSE = gutenberg.words('austen-sense.txt')\n",
    "BIBLE_KJV = gutenberg.words('bible-kjv.txt',)\n",
    "SHAKESPEARE_MACBETH = gutenberg.words('shakespeare-macbeth.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_text(url):\n",
    "    req = requests.get(url).text\n",
    "    resp = nltk.word_tokenize(req)\n",
    "    return nltk.Text(resp)\n",
    "    \n",
    "def long_words(text,is_url=False):\n",
    "    if is_url == True:\n",
    "        unique_text = set(url_text(text))\n",
    "        long_text = [w for w in unique_text if len(w)> 15]\n",
    "        return sorted(long_text)\n",
    "    else:\n",
    "        unique_text = set(text)\n",
    "        long_text = [w for w in unique_text if len(w)> 15]\n",
    "        return sorted(long_text)\n",
    "        \n",
    "def text_freq_dist(url):\n",
    "    unique_text = set(url_text(url))\n",
    "    fdist = FreqDist(url_text(url))\n",
    "    return sorted(w for w in unique_text if len(w) > 8 and fdist[w] > 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw_hw1 = long_words(HW1_URL,is_url=True)\n",
    "lw_ae = long_words(AUSTEN_EMMA,is_url=False)\n",
    "lw_ap = long_words(AUSTEN_PERSUASION,is_url=False)\n",
    "lw_as = long_words(AUSTEN_SENSE,is_url=False)\n",
    "lw_bible = long_words(BIBLE_KJV,is_url=False)\n",
    "lw_sm = long_words(SHAKESPEARE_MACBETH,is_url=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Number of Long Words</th>\n",
       "      <th>Long Words</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Distinct Word Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HW1_URL</td>\n",
       "      <td>8</td>\n",
       "      <td>[//www.gutenberg.org/2/4/6/4/24644/, Daffy-dow...</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austen Emma</td>\n",
       "      <td>10</td>\n",
       "      <td>[Disingenuousness, companionableness, disagree...</td>\n",
       "      <td>192427</td>\n",
       "      <td>7811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austen Persuasion</td>\n",
       "      <td>4</td>\n",
       "      <td>[acknowledgements, incomprehensible, misconstr...</td>\n",
       "      <td>98171</td>\n",
       "      <td>6132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Austen Sense</td>\n",
       "      <td>4</td>\n",
       "      <td>[companionableness, disinterestedness, disqual...</td>\n",
       "      <td>141576</td>\n",
       "      <td>6833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bible KJV</td>\n",
       "      <td>10</td>\n",
       "      <td>[Bashanhavothjair, Chepharhaammonai, Chushanri...</td>\n",
       "      <td>1010654</td>\n",
       "      <td>13769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shakespeare Macbeth</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>23140</td>\n",
       "      <td>4017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Text  Number of Long Words  \\\n",
       "0              HW1_URL                     8   \n",
       "1          Austen Emma                    10   \n",
       "2    Austen Persuasion                     4   \n",
       "3         Austen Sense                     4   \n",
       "4            Bible KJV                    10   \n",
       "5  Shakespeare Macbeth                     0   \n",
       "\n",
       "                                          Long Words  Word Count  \\\n",
       "0  [//www.gutenberg.org/2/4/6/4/24644/, Daffy-dow...          54   \n",
       "1  [Disingenuousness, companionableness, disagree...      192427   \n",
       "2  [acknowledgements, incomprehensible, misconstr...       98171   \n",
       "3  [companionableness, disinterestedness, disqual...      141576   \n",
       "4  [Bashanhavothjair, Chepharhaammonai, Chushanri...     1010654   \n",
       "5                                                 []       23140   \n",
       "\n",
       "   Distinct Word Count  \n",
       "0                   21  \n",
       "1                 7811  \n",
       "2                 6132  \n",
       "3                 6833  \n",
       "4                13769  \n",
       "5                 4017  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = pd.Series(['HW1_URL',len(lw_hw1),lw_hw1,len(HW1_URL),len(set(HW1_URL))])\n",
    "s2 = pd.Series(['Austen Emma',len(lw_ae),lw_ae,len(AUSTEN_EMMA),len(set(AUSTEN_EMMA))])\n",
    "s3 = pd.Series(['Austen Persuasion',len(lw_ap),lw_ap,len(AUSTEN_PERSUASION),len(set(AUSTEN_PERSUASION))])\n",
    "s4 = pd.Series(['Austen Sense',len(lw_as),lw_as,len(AUSTEN_SENSE),len(set(AUSTEN_SENSE))])\n",
    "s5 = pd.Series(['Bible KJV',len(lw_bible),lw_bible,len(BIBLE_KJV),len(set(BIBLE_KJV))])\n",
    "s6 = pd.Series(['Shakespeare Macbeth',len(lw_sm),lw_sm,len(SHAKESPEARE_MACBETH),len(set(SHAKESPEARE_MACBETH))])\n",
    "\n",
    "df = pd.DataFrame([list(s1), list(s2),list(s3),list(s4),list(s5),list(s6)],columns= [\"Text\", \"Number of Long Words\",'Long Words','Word Count','Distinct Word Count'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3:\n",
    "> Now create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to same graded texts you used in homework 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s6 = pd.Series(['HW1_URL',lexical_diversity(HW1_URL),len(lw_hw1),len(HW1_URL),len(set(HW1_URL))])\n",
    "s7 = pd.Series(['Austen Emma',lexical_diversity(AUSTEN_EMMA),len(lw_ae),len(AUSTEN_EMMA),len(set(AUSTEN_EMMA))])\n",
    "s8 = pd.Series(['Austen Persuasion',lexical_diversity(AUSTEN_PERSUASION),len(lw_ae),len(AUSTEN_PERSUASION),len(set(AUSTEN_PERSUASION))])\n",
    "s9 = pd.Series(['Austen Sense',lexical_diversity(AUSTEN_SENSE),len(lw_ae),len(AUSTEN_SENSE),len(set(AUSTEN_SENSE))])\n",
    "s10 = pd.Series(['Bible KJV',lexical_diversity(BIBLE_KJV),len(lw_ae),len(BIBLE_KJV),len(set(BIBLE_KJV))])\n",
    "s11 = pd.Series(['Shakespeare Macbeth',lexical_diversity(SHAKESPEARE_MACBETH),len(lw_ae),len(SHAKESPEARE_MACBETH),len(set(SHAKESPEARE_MACBETH))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ws = pd.DataFrame([list(s6), list(s7),list(s8),list(s9),list(s10),list(s11)],columns= [\"Text\", \"Lexical Diversity\",'Long Words',\"Length\",'Distinct Word Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = [\"Lexical Diversity\",'Long Words','Distinct Word Count']\n",
    "df_ws['Weighted Score'] = df_ws[col_list].sum(axis=1).div(len(col_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Lexical Diversity</th>\n",
       "      <th>Long Words</th>\n",
       "      <th>Length</th>\n",
       "      <th>Distinct Word Count</th>\n",
       "      <th>Weighted Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HW1_URL</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>9.796296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austen Emma</td>\n",
       "      <td>0.040592</td>\n",
       "      <td>10</td>\n",
       "      <td>192427</td>\n",
       "      <td>7811</td>\n",
       "      <td>2607.013531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austen Persuasion</td>\n",
       "      <td>0.062462</td>\n",
       "      <td>10</td>\n",
       "      <td>98171</td>\n",
       "      <td>6132</td>\n",
       "      <td>2047.354154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Austen Sense</td>\n",
       "      <td>0.048264</td>\n",
       "      <td>10</td>\n",
       "      <td>141576</td>\n",
       "      <td>6833</td>\n",
       "      <td>2281.016088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bible KJV</td>\n",
       "      <td>0.013624</td>\n",
       "      <td>10</td>\n",
       "      <td>1010654</td>\n",
       "      <td>13769</td>\n",
       "      <td>4593.004541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shakespeare Macbeth</td>\n",
       "      <td>0.173596</td>\n",
       "      <td>10</td>\n",
       "      <td>23140</td>\n",
       "      <td>4017</td>\n",
       "      <td>1342.391199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Text  Lexical Diversity  Long Words   Length  \\\n",
       "0              HW1_URL           0.388889           8       54   \n",
       "1          Austen Emma           0.040592          10   192427   \n",
       "2    Austen Persuasion           0.062462          10    98171   \n",
       "3         Austen Sense           0.048264          10   141576   \n",
       "4            Bible KJV           0.013624          10  1010654   \n",
       "5  Shakespeare Macbeth           0.173596          10    23140   \n",
       "\n",
       "   Distinct Word Count  Weighted Score  \n",
       "0                   21        9.796296  \n",
       "1                 7811     2607.013531  \n",
       "2                 6132     2047.354154  \n",
       "3                 6833     2281.016088  \n",
       "4                13769     4593.004541  \n",
       "5                 4017     1342.391199  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ws"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76cbc1d22d2ec03295cd5a8ae680c721667c569897428aaeeb69f9305902a57b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
