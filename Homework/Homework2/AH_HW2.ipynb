{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1\n",
    "#### DS7337: Natural Language Processing, Fall 2022\n",
    "By: Allen Hoskins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Section 1\n",
    "> In Python, create a method for scoring the vocabulary size of a text, and normalize the score from 0 to 1. It does not matter what method you use for normalization as long as you explain it in a short paragraph. \n",
    "\n",
    "\n",
    "> The below code uses sample code from NormalizationExample.ipynb. It uses both a created min/max scaler and Sklearn's `minmax_scale` to normalize the texts and produce a normalized score from 0 to 1. The `n_vocab_size` fuction, first take the length of the distinct count of words in each text and creates a variable `vocab_size` in an array. Once this is calculated for each text, normalization will go through both the created formula that takes the previous vocab size, subtracts the minimum and then divides by the difference in the minimum and maximum vocab size for all texts. The `sklearn` version (`minmax_scale`) uses the same calculation, but with less code and the results are show below with the exact same values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import text\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,minmax_scale\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Method to get the Vocab Size and normalize the score\n",
    "\n",
    "def n_vocab_size(*arg):\n",
    "    vocab_size = np.array([])\n",
    "    vocab_size_norm = np.array([])\n",
    "    \n",
    "    #### Getting the Vocab Size\n",
    "    for text in arg:\n",
    "        vocab_size = np.append(vocab_size,len(set(text)))\n",
    "    \n",
    "    #### Normalizing using the formula \n",
    "    for vsize in vocab_size:\n",
    "        vocab_size_norm = np.append(vocab_size_norm,(vsize - vocab_size.min()) /\n",
    "                                                    (vocab_size.max() - vocab_size.min()))\n",
    "    \n",
    "    #### Normalizing using sklearn preprocessing \n",
    "    vocab_size_norm_sklearn = minmax_scale(vocab_size, feature_range=(0,1), axis=0)\n",
    "    \n",
    "    return(vocab_size,vocab_size_norm,vocab_size_norm_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = n_vocab_size(text1,text2,text3,text4,text5,text6,text7,text8,text9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Values by using the formula\n",
      "- 1.0\n",
      "- 0.31440496457795597\n",
      "- 0.09231698610577187\n",
      "- 0.48970289417321106\n",
      "- 0.2722829370091713\n",
      "- 0.05810313581196112\n",
      "- 0.6205722444944808\n",
      "- 0.0\n",
      "- 0.31297709923664124\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalized Values by using the formula\", *vocab_size[1],sep='\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Values by using sklearn\n",
      "- 0.9999999999999999\n",
      "- 0.3144049645779559\n",
      "- 0.09231698610577188\n",
      "- 0.489702894173211\n",
      "- 0.2722829370091713\n",
      "- 0.05810313581196112\n",
      "- 0.6205722444944807\n",
      "- 0.0\n",
      "- 0.3129770992366412\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalized Values by using sklearn\", *vocab_size[2], sep='\\n- ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Section 2: \n",
    ">After consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above.\n",
    "\n",
    "> The below code utilizes example code from Chapter 1 section 3.2 of Bird-Klein to create the `long_words` function. This code is slightly altered to handle if the text is being pulled a url or from the `gutenberg` module of the NLTK package. In short, the function takes a distinct list of words from each text and then loops through each list to determine the total characters in each word. If the word is longer than 15 characters it is considered a \"long word\" and is returned when the function is called. Once the number of long words were determined, the results were run throught the function `norm_long_word`. This function, same as above, utilizes Sklearn's `minmax_scale` to normalize the number of long words returned in each text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW1_URL = 'https://www.gutenberg.org/cache/epub/24644/pg24644.txt'\n",
    "AUSTEN_EMMA = gutenberg.words('austen-emma.txt')\n",
    "AUSTEN_PERSUASION = gutenberg.words('austen-persuasion.txt')\n",
    "AUSTEN_SENSE = gutenberg.words('austen-sense.txt')\n",
    "BIBLE_KJV = gutenberg.words('bible-kjv.txt',)\n",
    "SHAKESPEARE_MACBETH = gutenberg.words('shakespeare-macbeth.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_text(url):\n",
    "    req = requests.get(url).text\n",
    "    resp = nltk.word_tokenize(req)\n",
    "    return nltk.Text(resp)\n",
    "    \n",
    "def long_words(text,is_url=False):\n",
    "    if is_url == True:\n",
    "        unique_text = set(url_text(text))\n",
    "        long_text = [w for w in unique_text if len(w)> 15]\n",
    "        return sorted(long_text)\n",
    "    else:\n",
    "        unique_text = set(text)\n",
    "        long_text = [w for w in unique_text if len(w)> 15]\n",
    "        return sorted(long_text)\n",
    "\n",
    "def text_freq_dist(url):\n",
    "    unique_text = set(url_text(url))\n",
    "    fdist = FreqDist(url_text(url))\n",
    "    return sorted(w for w in unique_text if len(w) > 8 and fdist[w] > 7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw_hw1 = long_words(HW1_URL,is_url=True)\n",
    "lw_ae = long_words(AUSTEN_EMMA,is_url=False)\n",
    "lw_ap = long_words(AUSTEN_PERSUASION,is_url=False)\n",
    "lw_as = long_words(AUSTEN_SENSE,is_url=False)\n",
    "lw_bible = long_words(BIBLE_KJV,is_url=False)\n",
    "lw_sm = long_words(SHAKESPEARE_MACBETH,is_url=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_long_word(*arg):\n",
    "    vocab_size_long_word = np.array([])\n",
    "    vocab_size_long_word_norm = np.array([])\n",
    "    \n",
    "    #### Getting the Vocab Size\n",
    "    for item in arg:\n",
    "        vocab_size_long_word = np.append(vocab_size_long_word,len(item))\n",
    "    \n",
    "    #### Normalizing using sklearn preprocessing \n",
    "    vocab_size_long_word_norm = minmax_scale(vocab_size_long_word, feature_range=(0,1), axis=0)\n",
    "    \n",
    "    return(vocab_size_long_word,vocab_size_long_word_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec2_norm_long = norm_long_word(lw_hw1,lw_ae,lw_ap,lw_as,lw_bible,lw_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Number of Long Words</th>\n",
       "      <th>Long Words</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Distinct Word Count</th>\n",
       "      <th>Sklearn Normalized Long Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HW1_URL</td>\n",
       "      <td>8</td>\n",
       "      <td>[//www.gutenberg.org/2/4/6/4/24644/, Daffy-dow...</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austen Emma</td>\n",
       "      <td>10</td>\n",
       "      <td>[Disingenuousness, companionableness, disagree...</td>\n",
       "      <td>192427</td>\n",
       "      <td>7811</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austen Persuasion</td>\n",
       "      <td>4</td>\n",
       "      <td>[acknowledgements, incomprehensible, misconstr...</td>\n",
       "      <td>98171</td>\n",
       "      <td>6132</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Austen Sense</td>\n",
       "      <td>4</td>\n",
       "      <td>[companionableness, disinterestedness, disqual...</td>\n",
       "      <td>141576</td>\n",
       "      <td>6833</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bible KJV</td>\n",
       "      <td>10</td>\n",
       "      <td>[Bashanhavothjair, Chepharhaammonai, Chushanri...</td>\n",
       "      <td>1010654</td>\n",
       "      <td>13769</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shakespeare Macbeth</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>23140</td>\n",
       "      <td>4017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Text  Number of Long Words  \\\n",
       "0              HW1_URL                     8   \n",
       "1          Austen Emma                    10   \n",
       "2    Austen Persuasion                     4   \n",
       "3         Austen Sense                     4   \n",
       "4            Bible KJV                    10   \n",
       "5  Shakespeare Macbeth                     0   \n",
       "\n",
       "                                          Long Words  Word Count  \\\n",
       "0  [//www.gutenberg.org/2/4/6/4/24644/, Daffy-dow...          54   \n",
       "1  [Disingenuousness, companionableness, disagree...      192427   \n",
       "2  [acknowledgements, incomprehensible, misconstr...       98171   \n",
       "3  [companionableness, disinterestedness, disqual...      141576   \n",
       "4  [Bashanhavothjair, Chepharhaammonai, Chushanri...     1010654   \n",
       "5                                                 []       23140   \n",
       "\n",
       "   Distinct Word Count  Sklearn Normalized Long Words  \n",
       "0                   21                            0.8  \n",
       "1                 7811                            1.0  \n",
       "2                 6132                            0.4  \n",
       "3                 6833                            0.4  \n",
       "4                13769                            1.0  \n",
       "5                 4017                            0.0  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = pd.Series(['HW1_URL',len(lw_hw1),lw_hw1,len(HW1_URL),len(set(HW1_URL))])\n",
    "s2 = pd.Series(['Austen Emma',len(lw_ae),lw_ae,len(AUSTEN_EMMA),len(set(AUSTEN_EMMA))])\n",
    "s3 = pd.Series(['Austen Persuasion',len(lw_ap),lw_ap,len(AUSTEN_PERSUASION),len(set(AUSTEN_PERSUASION))])\n",
    "s4 = pd.Series(['Austen Sense',len(lw_as),lw_as,len(AUSTEN_SENSE),len(set(AUSTEN_SENSE))])\n",
    "s5 = pd.Series(['Bible KJV',len(lw_bible),lw_bible,len(BIBLE_KJV),len(set(BIBLE_KJV))])\n",
    "s6 = pd.Series(['Shakespeare Macbeth',len(lw_sm),lw_sm,len(SHAKESPEARE_MACBETH),len(set(SHAKESPEARE_MACBETH))])\n",
    "\n",
    "df = pd.DataFrame([list(s1), list(s2),list(s3),list(s4),list(s5),list(s6)],columns= [\"Text\", \"Number of Long Words\",'Long Words','Word Count','Distinct Word Count'])\n",
    "\n",
    "df['Sklearn Normalized Long Words'] = sec2_norm_long[1].tolist()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Section 3:\n",
    "> Now create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to same graded texts you used in homework 1.\n",
    "\n",
    "> The below code utilizes the `lexical_diverstiy` function defined in Homework 1 as well as the `long_words` function defined in section 2 above. To create a \"text difficulty score\" with equal weighting, a data frame containing all of the function results was created. After creating the data frame `lexical_diversity`, `Sklearn Normalized Value`, and `Sklearn Normalized Long Words` were summed and then divided by the number of values (3). This calculation created a new `weighted score` that is able to be used to compare texts of all lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "s6 = pd.Series(['HW1_URL',lexical_diversity(HW1_URL),len(lw_hw1),len(HW1_URL),len(set(HW1_URL))])\n",
    "s7 = pd.Series(['Austen Emma',lexical_diversity(AUSTEN_EMMA),len(lw_ae),len(AUSTEN_EMMA),len(set(AUSTEN_EMMA))])\n",
    "s8 = pd.Series(['Austen Persuasion',lexical_diversity(AUSTEN_PERSUASION),len(lw_ae),len(AUSTEN_PERSUASION),len(set(AUSTEN_PERSUASION))])\n",
    "s9 = pd.Series(['Austen Sense',lexical_diversity(AUSTEN_SENSE),len(lw_ae),len(AUSTEN_SENSE),len(set(AUSTEN_SENSE))])\n",
    "s10 = pd.Series(['Bible KJV',lexical_diversity(BIBLE_KJV),len(lw_ae),len(BIBLE_KJV),len(set(BIBLE_KJV))])\n",
    "s11 = pd.Series(['Shakespeare Macbeth',lexical_diversity(SHAKESPEARE_MACBETH),len(lw_ae),len(SHAKESPEARE_MACBETH),len(set(SHAKESPEARE_MACBETH))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ws = pd.DataFrame([list(s6), list(s7),list(s8),list(s9),list(s10),list(s11)],columns= [\"Text\", \"Lexical Diversity\",'Long Words',\"Length\",'Distinct Word Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sect3_vocab_norm = n_vocab_size('HW1_URL','AUSTEN_EMMA','AUSTEN_PERSUASION','AUSTEN_SENSE','BIBLE_KJV','SHAKESPEARE_MACBETH')\n",
    "df_ws['Sklearn Normalized Value'] = sect3_vocab_norm[2].tolist()\n",
    "df_ws['Sklearn Normalized Long Words'] = sec2_norm_long[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = [\"Lexical Diversity\",'Sklearn Normalized Value','Sklearn Normalized Long Words']\n",
    "df_ws['Weighted Score'] = df_ws[col_list].sum(axis=1).div(len(col_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Lexical Diversity</th>\n",
       "      <th>Long Words</th>\n",
       "      <th>Length</th>\n",
       "      <th>Distinct Word Count</th>\n",
       "      <th>Sklearn Normalized Value</th>\n",
       "      <th>Sklearn Normalized Long Words</th>\n",
       "      <th>Weighted Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HW1_URL</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.396296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austen Emma</td>\n",
       "      <td>0.040592</td>\n",
       "      <td>10</td>\n",
       "      <td>192427</td>\n",
       "      <td>7811</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.413531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austen Persuasion</td>\n",
       "      <td>0.062462</td>\n",
       "      <td>10</td>\n",
       "      <td>98171</td>\n",
       "      <td>6132</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.420821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Austen Sense</td>\n",
       "      <td>0.048264</td>\n",
       "      <td>10</td>\n",
       "      <td>141576</td>\n",
       "      <td>6833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.149421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bible KJV</td>\n",
       "      <td>0.013624</td>\n",
       "      <td>10</td>\n",
       "      <td>1010654</td>\n",
       "      <td>13769</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.404541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shakespeare Macbeth</td>\n",
       "      <td>0.173596</td>\n",
       "      <td>10</td>\n",
       "      <td>23140</td>\n",
       "      <td>4017</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.391199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Text  Lexical Diversity  Long Words   Length  \\\n",
       "0              HW1_URL           0.388889           8       54   \n",
       "1          Austen Emma           0.040592          10   192427   \n",
       "2    Austen Persuasion           0.062462          10    98171   \n",
       "3         Austen Sense           0.048264          10   141576   \n",
       "4            Bible KJV           0.013624          10  1010654   \n",
       "5  Shakespeare Macbeth           0.173596          10    23140   \n",
       "\n",
       "   Distinct Word Count  Sklearn Normalized Value  \\\n",
       "0                   21                       0.0   \n",
       "1                 7811                       0.2   \n",
       "2                 6132                       0.8   \n",
       "3                 6833                       0.0   \n",
       "4                13769                       0.2   \n",
       "5                 4017                       1.0   \n",
       "\n",
       "   Sklearn Normalized Long Words  Weighted Score  \n",
       "0                            0.8        0.396296  \n",
       "1                            1.0        0.413531  \n",
       "2                            0.4        0.420821  \n",
       "3                            0.4        0.149421  \n",
       "4                            1.0        0.404541  \n",
       "5                            0.0        0.391199  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ws"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76cbc1d22d2ec03295cd5a8ae680c721667c569897428aaeeb69f9305902a57b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
